{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fdf5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers.language import LanguageParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89103642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d761f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use caminho absoluto para evitar ambiguidades em execuções diferentes\n",
    "repo_path = Path(\"/Users/yan/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/repo_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca986040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega arquivos .py do repositório usando um parser de linguagem\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    # Diretório raiz que contém o código-fonte a ser indexado\n",
    "    str(repo_path / \"libs/core/langchain_core\"),\n",
    "    # Padrão de busca: percorre recursivamente todos os caminhos\n",
    "    glob = \"**/*\",\n",
    "    # Extensões incluídas: apenas arquivos Python\n",
    "    suffixes = [\".py\"],\n",
    "    # Arquivos excluídos: evita problemas de encoding\n",
    "    exclude = [\"**/non-utf-8-encoding.py\"],\n",
    "    # Parser para extrair funções/classes e separar conteúdo\n",
    "    # 'parser_threshold=500' evita segmentação quando o arquivo é pequeno\n",
    "    parser = LanguageParser(language=\"python\", parser_threshold=500)\n",
    ")\n",
    "# Realiza a leitura e retorna uma lista de Document\n",
    "documents = loader.load()\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8b2392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide os documentos Python em chunks para melhor recuperação\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    # Linguagem alvo: enum Language.PYTHON (pode ser 'python')\n",
    "    language=Language.PYTHON,\n",
    "    # Tamanho máximo do chunk em caracteres\n",
    "    chunk_size=2000,\n",
    "    # Sobreposição entre chunks para preservar contexto\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Aplica a divisão e contabiliza o total de chunks\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a48452e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chave da OpenAI no ambiente para uso dos embeddings\n",
    "# Atenção: evite inserir chaves diretamente no código; use variáveis de ambiente\n",
    "# ou arquivos .env com python-dotenv em desenvolvimento\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a base vetorial (índice) em Chroma a partir dos textos\n",
    "# Usa embeddings da OpenAI; 'disallowed_special=()' não bloqueia tokens especiais\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "\n",
    "# Transforma a base em um 'retriever' para buscar contexto relevante\n",
    "retriever = db.as_retriever(\n",
    "    # 'mmr' (Maximal Marginal Relevance): equilibra relevância e diversidade\n",
    "    # Evita retornar documentos muito redundantes, aumentando a cobertura temática\n",
    "    search_type=\"mmr\",\n",
    "    # 'k' define o número de documentos finais retornados pelo retriever\n",
    "    # Valores comuns: 4–8; maiores valores aumentam recall mas podem trazer redundância\n",
    "    search_kwargs={\"k\": 8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o modelo de chat da OpenAI para gerar respostas\n",
    "# Observação: a classe correta é 'ChatOpenAI' (camel case)\n",
    "llm = ChatOpenAi(\n",
    "    # Nome do modelo de chat\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    # Limite de tokens da resposta do modelo\n",
    "    max_tokens=400,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o prompt do chat com mensagens de sistema e usuário\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # Mensagem de sistema: orienta o comportamento do assistente e injeta o contexto\n",
    "    (\"system\", \"You are a helpful assistant that answers questions about the document. {context}\"),\n",
    "    # Mensagem de usuário: consulta que será respondida pelo LLM\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Cadeia que 'stuffa' os documentos no prompt do LLM\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# Cadeia de recuperação: busca via retriever e passa contexto para a cadeia de documentos\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91730512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a cadeia de recuperação com a pergunta do usuário\n",
    "response = retrieval_chain.invoke({\"input\": \"Qual é o objetivo do projeto?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe a resposta final gerada pelo LLM, já com contexto\n",
    "print(response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
