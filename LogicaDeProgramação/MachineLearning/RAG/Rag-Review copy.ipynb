{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdf5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers.language import LanguageParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89103642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d761f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"/Users/yan/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/repo_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7fb4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=str(repo_path)) if not (repo_path / \".git\").exists() else Repo(str(repo_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca986040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega arquivos .py do repositório usando um parser de linguagem\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    # Diretório raiz que contém o código-fonte a ser indexado\n",
    "    str(repo_path / \"libs/core/langchain_core\"),\n",
    "    # Padrão de busca: percorre recursivamente todos os caminhos\n",
    "    glob = \"**/*\",\n",
    "    # Extensões incluídas: apenas arquivos Python\n",
    "    suffixes = [\".py\"],\n",
    "    # Arquivos excluídos: evita problemas de encoding\n",
    "    exclude = [\"**/non-utf-8-encoding.py\"],\n",
    "    # Parser para extrair funções/classes e separar conteúdo\n",
    "    # 'parser_threshold=500' evita segmentação quando o arquivo é pequeno\n",
    "    parser = LanguageParser(language=\"python\", parser_threshold=500)\n",
    ")\n",
    "# Realiza a leitura e retorna uma lista de Document\n",
    "documents = loader.load()\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8b2392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide os documentos Python em chunks para melhor recuperação\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    # Linguagem alvo: enum Language.PYTHON (pode ser 'python')\n",
    "    language=Language.PYTHON,\n",
    "    # Tamanho máximo do chunk em caracteres\n",
    "    chunk_size=2000,\n",
    "    # Sobreposição entre chunks para preservar contexto\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Aplica a divisão e contabiliza o total de chunks\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48452e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama não requer chave de API. Este passo não é necessário.\n",
    "# Caso esteja usando OpenAI em outro contexto, prefira variáveis de ambiente/.env\n",
    "# Exemplo (não necessário aqui): os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980bad4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":\"do embedding request: Post \\\"http://127.0.0.1:53830/embedding\\\": EOF\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cria a base vetorial (índice) em Chroma a partir dos textos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Usa embeddings locais via Ollama; 'nomic-embed-text' é modelo de embeddings\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Dica: rode 'ollama pull nomic-embed-text' antes, se necessário\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m db = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOllamaEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnomic-embed-text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:11434\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Transforma a base em um 'retriever' para buscar contexto relevante\u001b[39;00m\n\u001b[32m     11\u001b[39m retriever = db.as_retriever(\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 'mmr' (Maximal Marginal Relevance): equilibra relevância e diversidade\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Evita retornar documentos muito redundantes, aumentando a cobertura temática\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8\u001b[39m}\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1388\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[39m\n\u001b[32m   1386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1387\u001b[39m     ids = [doc.id \u001b[38;5;28;01mif\u001b[39;00m doc.id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchroma_cloud_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchroma_cloud_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1321\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m   1315\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m   1316\u001b[39m         api=chroma_collection._client,\n\u001b[32m   1317\u001b[39m         ids=ids,\n\u001b[32m   1318\u001b[39m         metadatas=metadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1319\u001b[39m         documents=texts,\n\u001b[32m   1320\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1324\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:623\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    625\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    626\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    627\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_community/embeddings/ollama.py:214\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m instruction_pairs = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.embed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_community/embeddings/ollama.py:202\u001b[39m, in \u001b[36mOllamaEmbeddings._embed\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    201\u001b[39m     iter_ = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Python/LogicaDeProgramação/MachineLearning/RAG/venv/lib/python3.13/site-packages/langchain_community/embeddings/ollama.py:176\u001b[39m, in \u001b[36mOllamaEmbeddings._process_emb_response\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    177\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m         % (res.status_code, res.text)\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    181\u001b[39m     t = res.json()\n",
      "\u001b[31mValueError\u001b[39m: Error raised by inference API HTTP code: 500, {\"error\":\"do embedding request: Post \\\"http://127.0.0.1:53830/embedding\\\": EOF\"}"
     ]
    }
   ],
   "source": [
    "# Cria a base vetorial (índice) em Chroma a partir dos textos\n",
    "# Usa embeddings locais via Ollama; 'nomic-embed-text' é modelo de embeddings\n",
    "# Dica: rode 'ollama pull nomic-embed-text' antes, se necessário\n",
    "import os\n",
    "os.environ[\"OLLAMA_HOST\"] = \"http://localhost:11434\"\n",
    "persist_dir = \"./chroma_db\"\n",
    "collection_name = \"langchain_core_py\"\n",
    "emb = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "# Valida conexão com Ollama e o modelo de embeddings\n",
    "_ = emb.embed_query(\"ping\")\n",
    "db = Chroma.from_documents(\n",
    "    texts,\n",
    "    emb,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# Transforma a base em um 'retriever' para buscar contexto relevante\n",
    "retriever = db.as_retriever(\n",
    "    # 'mmr' (Maximal Marginal Relevance): equilibra relevância e diversidade\n",
    "    # Evita retornar documentos muito redundantes, aumentando a cobertura temática\n",
    "    search_type=\"mmr\",\n",
    "    # 'k' define o número de documentos finais retornados pelo retriever\n",
    "    # Valores comuns: 4–8; maiores valores aumentam recall mas podem trazer redundância\n",
    "    search_kwargs={\"k\": 8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bb77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/x5tjl9qn4kq7zrm6vh3twr300000gn/T/ipykernel_66397/2910614059.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "# Configura o modelo local via Ollama para gerar respostas\n",
    "# Usando o modelo 'gemma3:4b' (precisa estar baixado no Ollama)\n",
    "# Dica: rode 'ollama pull gemma3:4b' antes, se necessário\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9100cd18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m document_chain = create_stuff_documents_chain(llm, prompt)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Cadeia de recuperação: busca via retriever e passa contexto para a cadeia de documentos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m retrieval_chain = create_retrieval_chain(\u001b[43mretriever\u001b[49m, document_chain)\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Define o prompt do chat com mensagens de sistema e usuário\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # Mensagem de sistema: orienta o comportamento do assistente e injeta o contexto\n",
    "    (\"system\", \"You are a helpful assistant that answers questions about the document. {context}\"),\n",
    "    # Mensagem de usuário: consulta que será respondida pelo LLM\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Cadeia que 'stuffa' os documentos no prompt do LLM\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# Cadeia de recuperação: busca via retriever e passa contexto para a cadeia de documentos\n",
    "if 'retriever' not in globals():\n",
    "    raise RuntimeError(\"Retriever não definido. Execute a célula de criação do índice (Chroma) e garanta que o Ollama esteja ativo e o modelo 'nomic-embed-text' baixado.\")\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91730512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a cadeia de recuperação com a pergunta do usuário\n",
    "response = retrieval_chain.invoke({\"input\": \"Qual é o objetivo do projeto?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe a resposta final gerada pelo LLM, já com contexto\n",
    "print(response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
