{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02223ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide textos em pedaços (chunks) controlando tamanho/overlap\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Embeddings e LLM da OpenAI (usados para vetorização e geração)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# Vetorstore local Chroma para armazenar/consultar embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# Constrói prompts de chat com variáveis (\"{question}\", \"{context}\")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Operadores de composição de cadeias (Passagem direta, Paralelo)\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "# Parser que transforma a saída do LLM em string simples\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Carrega PDF e divide em Documentos de página\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Retriever que mantém relação pai↔filho entre pedaços (parent/child)\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "# Loja de documentos em memória para armazenar os pais\n",
    "from langchain.storage import InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7947af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define a chave da OpenAI via variável de ambiente\n",
    "# As libs `OpenAIEmbeddings` e `ChatOpenAI` leem daqui automaticamente\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22efdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria função de embeddings da OpenAI (vetores para busca semântica)\n",
    "# Modelo: 'text-embedding-3-small' (rápido e barato, 1536 dimensões)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# LLM para gerar respostas; limita a saída para 200 tokens\n",
    "llm_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cceeb8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nome do arquivo PDF que será processado\n",
    "pdf = \"yan.pdf\"\n",
    "# Loader que lê o PDF e transforma em Documentos; sem extrair imagens\n",
    "carregar_pdf = PyPDFLoader(pdf, extract_images=False)\n",
    "# Carrega o PDF e divide por páginas em uma lista de Documentos\n",
    "pages = carregar_pdf.load_and_split()\n",
    "# Mostra quantas páginas foram carregadas\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f6d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter dos 'filhos' (pedaços pequenos usados na busca/vetorstore)\n",
    "# chunk_size=200: cada pedaço terá ~200 caracteres\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "# Splitter dos 'pais' (blocos maiores preservam contexto original)\n",
    "# chunk_size=4000: tamanho de cada bloco pai\n",
    "# chunk_overlap=200: sobreposição entre pedaços consecutivos para não perder contexto\n",
    "# length_function=len: mede tamanho via número de caracteres\n",
    "# add_start_index=True: inclui índice inicial de cada pedaço nos metadados\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa330c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/x5tjl9qn4kq7zrm6vh3twr300000gn/T/ipykernel_81055/3160842109.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstory = Chroma(embedding_function=embeddings, persist_directory=\"childVectorDB\")\n"
     ]
    }
   ],
   "source": [
    "# Armazena documentos-pai em memória (rápido, volátil)\n",
    "store = InMemoryStore()\n",
    "# Cria vetorstore local Chroma com embeddings OpenAI\n",
    "# persist_directory=\"childVectorDB\": caminho do banco local (reabre entre execuções)\n",
    "vectorstory = Chroma(embedding_function=embeddings, persist_directory=\"childVectorDB\")\n",
    "# Obs.: aviso de deprecação indica migrar para pacote langchain-chroma futuramente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ba8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever que cria pedaços 'filhos' para busca, mas retorna blocos 'pais'\n",
    "# vectorstore: onde os filhos (chunks) são indexados/vetorizados\n",
    "# docstore: onde os documentos-pai completos são guardados (InMemoryStore)\n",
    "# child_splitter: como quebrar o texto em pedaços menores (para indexação)\n",
    "# parent_splitter: como manter/recuperar o contexto maior (pais)\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstory,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Indexa as páginas do PDF no retriever (gera filhos e relaciona com pais)\n",
    "# ids=None: deixa o retriever criar IDs automaticamente\n",
    "parent_document_retriever.add_documents(pages, ids=None)\n",
    "# Dica: em retrievers baseados em Chroma simples, você pode usar search_type=\"mmr\"\n",
    "# para equilibrar relevância e diversidade nos resultados (evita retornos redundantes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeciona/obtém dados da coleção Chroma (útil para debug)\n",
    "parent_document_retriever.vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3f3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt com instrução e espaços para preencher pergunta e contexto\n",
    "# {question}: será a dúvida do usuário | {context}: textos recuperados\n",
    "TEMPLATE = \"\"\"\n",
    "Voce e um especialista em curriculos, responda a pergunta com base apenas no contexto fornecido.\n",
    "query: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "# Cria o objeto de prompt a partir do template acima\n",
    "rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9626af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara entradas paralelas para a cadeia: \n",
    "# - \"question\": repassa direto a pergunta do usuário\n",
    "# - \"context\": usa o retriever para buscar textos relevantes\n",
    "setup_retrival = RunnableParallel(\n",
    "    {\"question\": RunnablePassthrough(), \"context\": parent_document_retriever}\n",
    ")\n",
    "# Converte a saída do LLM para string simples (sem JSON/estruturas)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1299ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta a cadeia RAG:\n",
    "# 1) setup_retrival produz {question, context}\n",
    "# 2) rag_prompt injeta os dois no template\n",
    "# 3) llm_model gera a resposta\n",
    "# 4) output_parser extrai texto da resposta\n",
    "parent_chain_retrieval = setup_retrival | rag_prompt | llm_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a pergunta para a cadeia RAG; o retriever monta o contexto\n",
    "parent_chain_retrieval.invoke(\"Qual é a função do candidato?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
