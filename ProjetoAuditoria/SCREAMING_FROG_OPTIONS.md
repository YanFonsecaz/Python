# Op√ß√µes de Uso do Screaming Frog SEO Spider

O sistema de auditoria SEO oferece **duas op√ß√µes** para utilizar o Screaming Frog SEO Spider:

## üîß Op√ß√£o 1: Execu√ß√£o Autom√°tica via CLI

### Como Funciona
- O sistema executa automaticamente o Screaming Frog SEO Spider via linha de comando
- Realiza o crawling da URL fornecida em tempo real
- Processa os dados diretamente do output do crawler

### Como Usar
**Endpoint:** `POST /audit/start`

**Requisi√ß√£o JSON:**
```json
{
    "url": "https://exemplo.com",
    "options": {
        "include_crawler": true,
        "include_chrome_validations": true,
        "generate_documentation": true
    }
}
```

**Fluxo de Execu√ß√£o:**
1. Sistema valida a URL fornecida
2. Executa o Screaming Frog CLI automaticamente
3. Processa dados do crawling em tempo real
4. Consolida com dados de APIs (Google Analytics, Search Console)
5. Gera relat√≥rio completo

**Vantagens:**
- ‚úÖ Dados sempre atualizados
- ‚úÖ Processo totalmente automatizado
- ‚úÖ Integra√ß√£o com outras fontes de dados
- ‚úÖ N√£o requer conhecimento t√©cnico do usu√°rio

**Requisitos:**
- Screaming Frog SEO Spider instalado no servidor
- Licen√ßa v√°lida (para sites grandes)

---

## üìÅ Op√ß√£o 2: Upload de CSV Exportado

### Como Funciona
- Usu√°rio executa o Screaming Frog manualmente
- Exporta os dados como CSV
- Faz upload do arquivo via API
- Sistema processa os dados do CSV

### Como Usar
**Endpoint:** `POST /audit/start`

**Requisi√ß√£o Form-Data:**
```
Content-Type: multipart/form-data

screaming_frog_file: [arquivo.csv]
url: "https://exemplo.com" (opcional)
options: '{"generate_documentation": true}' (opcional)
```

**Exemplo com cURL:**
```bash
curl -X POST http://localhost:5000/audit/start \
  -F "screaming_frog_file=@crawl_data.csv" \
  -F "url=https://exemplo.com" \
  -F "options={\"generate_documentation\": true}"
```

**Fluxo de Execu√ß√£o:**
1. Sistema valida o arquivo CSV enviado
2. Carrega dados do Screaming Frog do arquivo
3. Analisa problemas baseados no CSV
4. Gera relat√≥rio focado nos dados do Screaming Frog
5. Opcionalmente gera documenta√ß√£o

**Vantagens:**
- ‚úÖ Controle total sobre o crawling
- ‚úÖ Pode usar configura√ß√µes personalizadas do Screaming Frog
- ‚úÖ Funciona offline
- ‚úÖ Permite an√°lise de crawls hist√≥ricos
- ‚úÖ N√£o requer Screaming Frog no servidor

**Requisitos:**
- Screaming Frog SEO Spider no computador do usu√°rio
- Arquivo CSV exportado do Screaming Frog

---

## üîÑ Como o Sistema Decide Entre as Op√ß√µes

O sistema detecta automaticamente qual op√ß√£o usar baseado na requisi√ß√£o:

```python
# No endpoint /audit/start
if 'screaming_frog_file' in request.files:
    # Op√ß√£o 2: Processa upload de CSV
    return _handle_csv_upload_audit()
else:
    # Op√ß√£o 1: Execu√ß√£o autom√°tica via CLI
    # Processa requisi√ß√£o JSON normal
```

## üìä Diferen√ßas nos Resultados

### Op√ß√£o 1 (CLI Autom√°tico)
- **Dados Consolidados:** Screaming Frog + Google APIs + Chrome DevTools
- **Relat√≥rio Completo:** Performance, SEO t√©cnico, Analytics
- **Tempo:** 5-10 minutos
- **Score:** Baseado em m√∫ltiplas fontes

### Op√ß√£o 2 (Upload CSV)
- **Dados Focados:** Apenas Screaming Frog
- **Relat√≥rio T√©cnico:** SEO t√©cnico detalhado
- **Tempo:** 3-8 minutos
- **Score:** Baseado nos problemas do Screaming Frog

## üõ†Ô∏è Configura√ß√µes Avan√ßadas

### Vari√°veis de Ambiente
```bash
# Para Op√ß√£o 1 (CLI)
SCREAMING_FROG_PATH=/path/to/screaming-frog
SCREAMING_FROG_TIMEOUT=300

# Para ambas as op√ß√µes
SCREAMING_FROG_MAX_PAGES=10000
SCREAMING_FROG_DEPTH=5
```

### Op√ß√µes Personalizadas
```json
{
    "screaming_frog_options": {
        "max_pages": 5000,
        "crawl_depth": 3,
        "include_external_links": false,
        "respect_robots_txt": true
    }
}
```

## üìã Formatos de CSV Suportados

O sistema aceita os seguintes exports do Screaming Frog:
- **Internal HTML:** P√°ginas internas do site
- **Response Codes:** Status HTTP das p√°ginas
- **Page Titles:** T√≠tulos das p√°ginas
- **Meta Description:** Meta descri√ß√µes
- **H1:** Cabe√ßalhos H1
- **Images:** An√°lise de imagens
- **External Links:** Links externos

## üöÄ Exemplos Pr√°ticos

### Exemplo 1: Auditoria Autom√°tica Completa
```bash
curl -X POST http://localhost:5000/audit/start \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://meusite.com",
    "options": {
      "include_crawler": true,
      "include_chrome_validations": true,
      "generate_documentation": true
    }
  }'
```

### Exemplo 2: Upload de CSV do Screaming Frog
```bash
curl -X POST http://localhost:5000/audit/start \
  -F "screaming_frog_file=@internal_html.csv" \
  -F "url=https://meusite.com"
```

## üìà Monitoramento

Ambas as op√ß√µes podem ser monitoradas atrav√©s dos endpoints:
- `GET /audit/status/{audit_id}` - Status da auditoria
- `GET /audit/result/{audit_id}` - Resultado completo
- `GET /audit/report/{audit_id}` - Relat√≥rio formatado

## üîç Troubleshooting

### Problemas Comuns - Op√ß√£o 1 (CLI)
- **Screaming Frog n√£o encontrado:** Verificar `SCREAMING_FROG_PATH`
- **Timeout:** Aumentar `SCREAMING_FROG_TIMEOUT`
- **Licen√ßa:** Verificar licen√ßa para sites grandes

### Problemas Comuns - Op√ß√£o 2 (CSV)
- **Arquivo inv√°lido:** Verificar se √© CSV v√°lido do Screaming Frog
- **Formato incorreto:** Usar export padr√£o do Screaming Frog
- **Arquivo muito grande:** Limitar crawl ou dividir em partes

---

**üí° Dica:** Para sites pequenos (< 500 p√°ginas), use a Op√ß√£o 1 para an√°lise completa. Para sites grandes ou an√°lises espec√≠ficas, use a Op√ß√£o 2 com configura√ß√µes personalizadas do Screaming Frog.